{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "name": "AI-PSCI-017: Model Validation & Performance Metrics",
      "toc_visible": true,
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "header"
      },
      "source": [
        "# AI-PSCI-017: Model Validation & Performance Metrics\n",
        "\n",
        "**AI in Pharmaceutical Sciences: Bench to Bedside**  \n",
        "VCU School of Pharmacy | VIP Program | Spring 2026\n",
        "\n",
        "---\n",
        "\n",
        "**Week 10 | Module: Testing & Evaluation | Estimated Time: 90-120 minutes**\n",
        "\n",
        "**Prerequisites**: AI-PSCI-015 (AutoDock Vina), AI-PSCI-016 (DiffDock & GNINA)\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "objectives"
      },
      "source": [
        "## ðŸŽ¯ Learning Objectives\n",
        "\n",
        "After completing this talktorial, you will be able to:\n",
        "\n",
        "1. **Systematically compare** docking and prediction methods using quantitative metrics\n",
        "2. **Calculate and interpret RMSD** to assess pose prediction accuracy\n",
        "3. **Analyze scoring correlations** between different docking methods\n",
        "4. **Design validation experiments** with appropriate controls and statistics\n",
        "5. **Make evidence-based recommendations** for method selection in drug discovery\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "background"
      },
      "source": [
        "## ðŸ“š Background\n",
        "\n",
        "### Why Validate Computational Methods?\n",
        "\n",
        "In drug discovery, computational predictions guide expensive experimental decisions. A docking method that produces incorrect poses could lead to:\n",
        "- Wasted synthesis efforts on compounds that don't actually bind\n",
        "- Missed opportunities from discarding good candidates\n",
        "- Incorrect structure-activity relationship (SAR) interpretations\n",
        "\n",
        "**Validation** ensures our computational tools are fit for purpose before we rely on them for novel predictions.\n",
        "\n",
        "### The Three Docking Methods\n",
        "\n",
        "You've now used three different docking approaches:\n",
        "\n",
        "| Method | Type | Key Feature | Scoring |\n",
        "|--------|------|-------------|--------|\n",
        "| **AutoDock Vina** | Physics-based | Fast, widely validated | kcal/mol (more negative = better) |\n",
        "| **DiffDock** | AI (Diffusion) | Blind docking, no pocket needed | Confidence (0-1, higher = better) |\n",
        "| **GNINA** | AI (CNN) | Neural network rescoring | CNN score (0-1) + affinity (pKd) |\n",
        "\n",
        "### Key Validation Metrics\n",
        "\n",
        "- **RMSD (Root Mean Square Deviation)**: Measures how closely a predicted pose matches the experimental structure. Lower is better.\n",
        "  - < 2.0 Ã…: **Excellent** (successful pose prediction)\n",
        "  - 2.0 - 4.0 Ã…: **Fair** (correct binding mode, some deviation)\n",
        "  - > 4.0 Ã…: **Poor** (different binding mode or failed prediction)\n",
        "\n",
        "- **Success Rate**: Percentage of predictions with RMSD < 2.0 Ã…\n",
        "\n",
        "- **Scoring Correlation**: How well does the score predict experimental binding affinity?\n",
        "\n",
        "### Key Concepts\n",
        "- **RMSD**: Root Mean Square Deviation - geometric measure of pose accuracy\n",
        "- **Cross-validation**: Testing on data not used for method development\n",
        "- **Benchmarking**: Systematic comparison against known standards\n",
        "- **Statistical significance**: Ensuring differences aren't due to chance\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setup-header"
      },
      "source": [
        "## ðŸ› ï¸ Setup\n",
        "\n",
        "Run this cell to install required packages:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "setup-install",
        "cellView": "form"
      },
      "source": [
        "#@title ðŸ› ï¸ Install Required Packages\n",
        "!pip install rdkit biopython py3Dmol scipy -q\n",
        "print(\"âœ… Packages installed successfully!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "imports-header"
      },
      "source": [
        "Import required libraries:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "imports"
      },
      "source": [
        "# Standard imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import stats\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# BioPython for structure analysis\n",
        "from Bio.PDB import PDBParser, Superimposer\n",
        "from Bio.PDB.PDBIO import PDBIO\n",
        "\n",
        "# RDKit for molecular operations\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import AllChem, Draw\n",
        "\n",
        "print(\"âœ… Libraries imported successfully!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "target-config-header"
      },
      "source": [
        "---\n",
        "\n",
        "## ðŸŽ¯ Target Configuration\n",
        "\n",
        "Select your drug target. This should match your selection from AI-PSCI-015 and AI-PSCI-016."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "target-config",
        "cellView": "form"
      },
      "source": [
        "#@title ðŸŽ¯ Select Your Drug Target\n",
        "TARGET = \"DHFR\" #@param [\"DHFR\", \"ABL1\", \"EGFR\", \"AChE\", \"COX-2\", \"DPP-4\"]\n",
        "\n",
        "# Complete target configuration with all identifiers\n",
        "TARGET_CONFIG = {\n",
        "    \"DHFR\": {\n",
        "        \"pdb\": \"2W9S\",\n",
        "        \"uniprot\": \"P0ABQ4\",\n",
        "        \"chembl\": \"CHEMBL202\",\n",
        "        \"drug\": \"Trimethoprim\",\n",
        "        \"drug_smiles\": \"COc1cc(Cc2cnc(N)nc2N)cc(OC)c1OC\",\n",
        "        \"ligand_code\": \"TOP\",\n",
        "        \"expected_affinity\": -8.5  # Approximate Ki ~0.5 nM\n",
        "    },\n",
        "    \"ABL1\": {\n",
        "        \"pdb\": \"1IEP\",\n",
        "        \"uniprot\": \"P00519\",\n",
        "        \"chembl\": \"CHEMBL1862\",\n",
        "        \"drug\": \"Imatinib\",\n",
        "        \"drug_smiles\": \"Cc1ccc(NC(=O)c2ccc(CN3CCN(C)CC3)cc2)cc1Nc1nccc(-c2cccnc2)n1\",\n",
        "        \"ligand_code\": \"STI\",\n",
        "        \"expected_affinity\": -10.5  # Ki ~37 nM\n",
        "    },\n",
        "    \"EGFR\": {\n",
        "        \"pdb\": \"1M17\",\n",
        "        \"uniprot\": \"P00533\",\n",
        "        \"chembl\": \"CHEMBL203\",\n",
        "        \"drug\": \"Erlotinib\",\n",
        "        \"drug_smiles\": \"COCCOc1cc2ncnc(Nc3cccc(C#C)c3)c2cc1OCCOC\",\n",
        "        \"ligand_code\": \"AQ4\",\n",
        "        \"expected_affinity\": -9.8  # Ki ~2 nM\n",
        "    },\n",
        "    \"AChE\": {\n",
        "        \"pdb\": \"4EY7\",\n",
        "        \"uniprot\": \"P22303\",\n",
        "        \"chembl\": \"CHEMBL220\",\n",
        "        \"drug\": \"Donepezil\",\n",
        "        \"drug_smiles\": \"COc1cc2CC(CC2cc1OC)CN1CCc2ccccc2C1=O\",\n",
        "        \"ligand_code\": \"E20\",\n",
        "        \"expected_affinity\": -10.2  # Ki ~6 nM\n",
        "    },\n",
        "    \"COX-2\": {\n",
        "        \"pdb\": \"3LN1\",\n",
        "        \"uniprot\": \"P35354\",\n",
        "        \"chembl\": \"CHEMBL230\",\n",
        "        \"drug\": \"Celecoxib\",\n",
        "        \"drug_smiles\": \"Cc1ccc(-c2cc(C(F)(F)F)nn2-c2ccc(S(N)(=O)=O)cc2)cc1\",\n",
        "        \"ligand_code\": \"CEL\",\n",
        "        \"expected_affinity\": -9.0  # Ki ~40 nM\n",
        "    },\n",
        "    \"DPP-4\": {\n",
        "        \"pdb\": \"1X70\",\n",
        "        \"uniprot\": \"P27487\",\n",
        "        \"chembl\": \"CHEMBL284\",\n",
        "        \"drug\": \"Sitagliptin\",\n",
        "        \"drug_smiles\": \"Fc1cc(F)c(C[C@H](N)CC(=O)N2CCn3c(nnc3C(F)(F)F)C2)c(F)c1F\",\n",
        "        \"ligand_code\": \"715\",\n",
        "        \"expected_affinity\": -9.5  # Ki ~18 nM\n",
        "    }\n",
        "}\n",
        "\n",
        "config = TARGET_CONFIG[TARGET]\n",
        "print(f\"âœ… Target: {TARGET}\")\n",
        "print(f\"   PDB: {config['pdb']} | UniProt: {config['uniprot']}\")\n",
        "print(f\"   Reference Drug: {config['drug']}\")\n",
        "print(f\"   Ligand Code in PDB: {config['ligand_code']}\")\n",
        "print(f\"   Expected Affinity: {config['expected_affinity']} kcal/mol\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gi1-header"
      },
      "source": [
        "---\n",
        "\n",
        "## ðŸ”¬ Guided Inquiry 1: Preparing Validation Data\n",
        "\n",
        "### Context\n",
        "\n",
        "Before we can compare methods, we need to organize our docking results. In AI-PSCI-015 and AI-PSCI-016, you generated poses using AutoDock Vina, DiffDock, and GNINA.\n",
        "\n",
        "For this validation exercise, we'll simulate having those results and use the crystal structure as our ground truth. In a real workflow, you would load your actual docking output files.\n",
        "\n",
        "### Your Task\n",
        "\n",
        "Using your AI assistant, write code to:\n",
        "1. Download the crystal structure for your target\n",
        "2. Extract the bound ligand as the reference pose\n",
        "3. Create a DataFrame to organize docking results from all three methods\n",
        "4. Include scores, RMSD values, and method metadata\n",
        "\n",
        "ðŸ’¡ **Prompting Tips**:\n",
        "- Ask: \"How do I extract a ligand from a PDB file using BioPython?\"\n",
        "- If you have actual docking output files, ask how to parse SDF or PDBQT files\n",
        "- Request help organizing multi-method comparison data\n",
        "\n",
        "### Verification\n",
        "After running your code, confirm:\n",
        "- [ ] Crystal structure downloaded successfully\n",
        "- [ ] Reference ligand extracted with correct number of atoms\n",
        "- [ ] DataFrame created with columns for each method's scores and RMSD\n",
        "\n",
        "ðŸ““ **Lab Notebook**: Document the source of your validation data and any preprocessing steps applied."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gi1-code"
      },
      "source": [
        "# Your code here\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gi2-header"
      },
      "source": [
        "---\n",
        "\n",
        "## ðŸ”¬ Guided Inquiry 2: RMSD Analysis\n",
        "\n",
        "### Context\n",
        "\n",
        "RMSD (Root Mean Square Deviation) is the gold standard for assessing docking pose accuracy. It measures the average distance between corresponding atoms in two structures.\n",
        "\n",
        "**RMSD Formula**:\n",
        "$$RMSD = \\sqrt{\\frac{1}{N} \\sum_{i=1}^{N} |r_i^{pred} - r_i^{ref}|^2}$$\n",
        "\n",
        "Where:\n",
        "- $N$ = number of atoms\n",
        "- $r_i^{pred}$ = predicted position of atom $i$\n",
        "- $r_i^{ref}$ = reference (crystal) position of atom $i$\n",
        "\n",
        "### Your Task\n",
        "\n",
        "Using your AI assistant, write code to:\n",
        "1. Visualize the RMSD distribution for each docking method\n",
        "2. Calculate summary statistics (mean, median, std) for each method\n",
        "3. Determine the success rate (RMSD < 2.0 Ã…) for each method\n",
        "4. Create a box plot comparing RMSD across methods\n",
        "\n",
        "ðŸ’¡ **Prompting Tips**:\n",
        "- Ask: \"How do I create side-by-side box plots with matplotlib?\"\n",
        "- Request help adding a horizontal line at the 2.0 Ã… threshold\n",
        "- Ask about statistical tests to compare distributions\n",
        "\n",
        "### Verification\n",
        "After running your code, confirm:\n",
        "- [ ] Box plot shows clear comparison between all three methods\n",
        "- [ ] 2.0 Ã… threshold line is visible\n",
        "- [ ] Success rates calculated for each method\n",
        "- [ ] Summary statistics table generated\n",
        "\n",
        "ðŸ““ **Lab Notebook**: Record which method has the highest success rate. Is this consistent with the published literature?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gi2-code"
      },
      "source": [
        "# Your code here\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gi3-header"
      },
      "source": [
        "---\n",
        "\n",
        "## ðŸ”¬ Guided Inquiry 3: Scoring Correlation Analysis\n",
        "\n",
        "### Context\n",
        "\n",
        "A good docking method should not only predict correct poses but also rank them correctly. The best pose should have the best score. We assess this by examining:\n",
        "\n",
        "1. **Score-RMSD correlation**: Do better scores correlate with lower RMSD?\n",
        "2. **Method agreement**: Do different methods agree on what's a good pose?\n",
        "3. **Rank correlation**: Do methods agree on the rank order?\n",
        "\n",
        "### Your Task\n",
        "\n",
        "Using your AI assistant, write code to:\n",
        "1. Create scatter plots of Score vs RMSD for each method\n",
        "2. Calculate Pearson and Spearman correlations\n",
        "3. Analyze whether the top-ranked pose is also the best RMSD\n",
        "4. Compare rank order agreement between methods\n",
        "\n",
        "ðŸ’¡ **Prompting Tips**:\n",
        "- Ask: \"What's the difference between Pearson and Spearman correlation?\"\n",
        "- Note that Vina scores are negative (more negative = better) while CNN scores are positive (higher = better)\n",
        "- Request help with correlation coefficient interpretation\n",
        "\n",
        "### Verification\n",
        "After running your code, confirm:\n",
        "- [ ] Scatter plots show Score vs RMSD for each method\n",
        "- [ ] Correlation coefficients calculated and interpreted\n",
        "- [ ] Top-1 accuracy reported (is best-scored pose also best RMSD?)\n",
        "\n",
        "ðŸ““ **Lab Notebook**: Which method shows the best score-RMSD correlation? What does this mean for virtual screening?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gi3-code"
      },
      "source": [
        "# Your code here\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gi4-header"
      },
      "source": [
        "---\n",
        "\n",
        "## ðŸ”¬ Guided Inquiry 4: Statistical Method Comparison\n",
        "\n",
        "### Context\n",
        "\n",
        "When comparing methods, we need to determine if observed differences are statistically significant or could have occurred by chance. Common tests include:\n",
        "\n",
        "- **Paired t-test**: Compare two methods on same test cases\n",
        "- **Wilcoxon signed-rank test**: Non-parametric alternative\n",
        "- **ANOVA**: Compare multiple methods simultaneously\n",
        "\n",
        "### Your Task\n",
        "\n",
        "Using your AI assistant, write code to:\n",
        "1. Perform statistical tests comparing RMSD distributions between methods\n",
        "2. Calculate 95% confidence intervals for mean RMSD\n",
        "3. Create a visualization showing statistical significance\n",
        "4. Report p-values with appropriate interpretation\n",
        "\n",
        "ðŸ’¡ **Prompting Tips**:\n",
        "- Ask: \"How do I perform a paired t-test in Python with scipy?\"\n",
        "- Ask about assumptions for parametric vs non-parametric tests\n",
        "- Request help with multiple comparison correction (e.g., Bonferroni)\n",
        "\n",
        "### Verification\n",
        "After running your code, confirm:\n",
        "- [ ] Pairwise comparisons performed for all method pairs\n",
        "- [ ] P-values reported with significance interpretation\n",
        "- [ ] Confidence intervals calculated\n",
        "- [ ] Visualization clearly shows significant differences\n",
        "\n",
        "ðŸ““ **Lab Notebook**: Are the differences between methods statistically significant? What sample size would you need for more confident conclusions?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gi4-code"
      },
      "source": [
        "# Your code here\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gi5-header"
      },
      "source": [
        "---\n",
        "\n",
        "## ðŸ”¬ Guided Inquiry 5: Method Performance Dashboard\n",
        "\n",
        "### Context\n",
        "\n",
        "A comprehensive performance dashboard helps stakeholders quickly assess method capabilities. This is essential for:\n",
        "- Choosing methods for production workflows\n",
        "- Communicating results to medicinal chemists\n",
        "- Documenting method validation in publications\n",
        "\n",
        "### Your Task\n",
        "\n",
        "Using your AI assistant, write code to:\n",
        "1. Create a multi-panel dashboard comparing all metrics\n",
        "2. Include radar/spider chart for method comparison\n",
        "3. Add a summary table with all key metrics\n",
        "4. Generate a single publication-quality figure\n",
        "\n",
        "ðŸ’¡ **Prompting Tips**:\n",
        "- Ask: \"How do I create a radar chart in matplotlib?\"\n",
        "- Request help with subplot layout for complex figures\n",
        "- Ask about color-blind friendly palettes\n",
        "\n",
        "### Verification\n",
        "After running your code, confirm:\n",
        "- [ ] Dashboard contains multiple informative panels\n",
        "- [ ] Radar chart shows relative method strengths\n",
        "- [ ] All metrics are clearly labeled\n",
        "- [ ] Figure saved at publication quality (300 dpi)\n",
        "\n",
        "ðŸ““ **Lab Notebook**: What are the key takeaways from your dashboard? Which method would you recommend for your target?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gi5-code"
      },
      "source": [
        "# Your code here\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gi6-header"
      },
      "source": [
        "---\n",
        "\n",
        "## ðŸ”¬ Guided Inquiry 6: Evidence-Based Recommendations\n",
        "\n",
        "### Context\n",
        "\n",
        "The ultimate goal of validation is to make informed decisions. Based on your analysis, you should be able to:\n",
        "- Recommend the best method for your specific application\n",
        "- Justify the recommendation with evidence\n",
        "- Identify scenarios where different methods might be preferred\n",
        "\n",
        "### Your Task\n",
        "\n",
        "Using your AI assistant, write code to:\n",
        "1. Create a decision framework based on your validation results\n",
        "2. Generate a recommendation report with supporting evidence\n",
        "3. Include caveats and limitations\n",
        "4. Suggest next steps for production use\n",
        "\n",
        "ðŸ’¡ **Prompting Tips**:\n",
        "- Ask: \"How do I create a formatted markdown report in Python?\"\n",
        "- Think about different use cases: virtual screening vs. lead optimization\n",
        "- Consider computational cost vs. accuracy tradeoffs\n",
        "\n",
        "### Verification\n",
        "After running your code, confirm:\n",
        "- [ ] Clear recommendation with primary and secondary choices\n",
        "- [ ] Evidence cited for each recommendation\n",
        "- [ ] Limitations and caveats documented\n",
        "- [ ] Report saved as markdown file\n",
        "\n",
        "ðŸ““ **Lab Notebook**: What additional validation would you perform before using these methods in a real drug discovery project?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gi6-code"
      },
      "source": [
        "# Your code here\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "checkpoint"
      },
      "source": [
        "---\n",
        "\n",
        "## âœ… Checkpoint\n",
        "\n",
        "Before moving on to the next talktorial, confirm you can:\n",
        "\n",
        "- [ ] Calculate RMSD between predicted and reference poses\n",
        "- [ ] Interpret RMSD thresholds (< 2.0 Ã… = success)\n",
        "- [ ] Perform statistical comparisons between methods\n",
        "- [ ] Create publication-quality validation figures\n",
        "- [ ] Generate evidence-based method recommendations\n",
        "- [ ] Document limitations and caveats honestly\n",
        "\n",
        "### Your lab notebook should include:\n",
        "- [ ] RMSD summary statistics for each method\n",
        "- [ ] Statistical test results and p-values\n",
        "- [ ] Your method recommendation with supporting evidence\n",
        "- [ ] Discussion of limitations and next steps\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "reflection"
      },
      "source": [
        "## ðŸ¤” Reflection Questions\n",
        "\n",
        "Answer these in your lab notebook:\n",
        "\n",
        "1. **Method Selection**: If you could only use ONE docking method for your research project, which would you choose and why? Consider accuracy, speed, and reliability.\n",
        "\n",
        "2. **Validation Strategy**: What additional experiments would you perform to increase confidence in your method selection before using it for novel compound predictions?\n",
        "\n",
        "3. **Limitations**: What are the potential pitfalls of relying solely on RMSD for docking validation? What other metrics might be informative?\n",
        "\n",
        "4. **Translation to Practice**: How would you communicate these validation results to a medicinal chemistry team who will use the predictions to guide synthesis decisions?\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "further-reading"
      },
      "source": [
        "## ðŸ“– Further Reading\n",
        "\n",
        "- [Warren et al. (2006) - A Critical Assessment of Docking Programs and Scoring Functions](https://doi.org/10.1021/jm050362n) - Classic benchmarking study\n",
        "- [Bender & CortÃ©s-Ciriano (2021) - Artificial Intelligence in Drug Discovery](https://doi.org/10.1038/s41573-021-00290-8) - Review of AI methods including DiffDock\n",
        "- [McNutt et al. (2021) - GNINA 1.0: Molecular Docking with Deep Learning](https://doi.org/10.26434/chemrxiv.13578635) - GNINA methodology paper\n",
        "- [Corso et al. (2023) - DiffDock: Diffusion Steps, Twists, and Turns for Molecular Docking](https://arxiv.org/abs/2210.01776) - DiffDock methodology\n",
        "- [Cross et al. (2009) - Comparison of Several Molecular Docking Programs](https://doi.org/10.1021/ci800324m) - Comprehensive docking comparison\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "research-connection"
      },
      "source": [
        "## ðŸ”— Connection to Research\n",
        "\n",
        "Method validation is a critical but often underappreciated step in computational drug discovery. The skills you've developed here directly translate to:\n",
        "\n",
        "**Academic Research**:\n",
        "- Benchmarking new algorithms against established methods\n",
        "- Publishing validation studies for novel targets\n",
        "- Supporting computational predictions with statistical evidence\n",
        "\n",
        "**Industry Applications**:\n",
        "- Qualifying computational methods for production use\n",
        "- Satisfying regulatory requirements for in silico evidence\n",
        "- Making resource allocation decisions (which method to run at scale)\n",
        "\n",
        "**Your Target**:\n",
        "Having validated docking methods for your specific target, you now have:\n",
        "- Confidence in which method to use for novel compound predictions\n",
        "- Understanding of expected accuracy and failure modes\n",
        "- Documentation to support your computational approach\n",
        "\n",
        "This validation framework can be applied to any computational method in drug discovery: ADMET prediction, binding affinity scoring, protein structure prediction, and more.\n",
        "\n",
        "---\n",
        "\n",
        "*AI-PSCI-017 Complete. Proceed to AI-PSCI-018: Debugging & Optimization Strategies.*"
      ]
    }
  ]
}
